"""
Enterprise Forecasting Platform - Professional Edition
AI-Powered Demand Forecasting System with Advanced Analytics

Version: 2.0.0
Last Updated: 2025-12-09
"""
from __future__ import annotations
import io
import numpy as np
import plotly.graph_objects as go
import logging
from pathlib import Path
import requests
import streamlit as st
import pandas as pd
import tempfile
import os
from datetime import datetime, date, timedelta
from dateutil.relativedelta import relativedelta
import holidays

# Constants
DATA_MIN = 50  # Minimum de points de donn√©es requis

# Configuration - Simplified for Streamlit Cloud
IS_STREAMLIT_CLOUD = os.getenv("STREAMLIT_RUNTIME_ENV") == "cloud"

# Simple logger setup
logger = logging.getLogger("DataVizApp")
logger.setLevel(logging.INFO)
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter("[%(asctime)s] [%(levelname)s] %(message)s", datefmt="%Y-%m-%d %H:%M:%S")
    handler.setFormatter(formatter)
    logger.addHandler(handler)

# Configuration API Modal
try:
    MODAL_API_URL = st.secrets["MODAL_API_URL"]
except (KeyError, FileNotFoundError):
    MODAL_API_URL = "https://hichemsaada0--forecast-api-predict-api.modal.run"

# =========================
# Fonctions utilitaires
# =========================

def prepare_daily_df(df, col_article="Description article", col_date="Date de livraison", col_qte="Quantite"):
    """Pr√©pare un DataFrame avec 1 ligne par (article, date) et quantit√©s = 0 si absence."""
    df[col_date] = pd.to_datetime(df[col_date], dayfirst=True, errors="coerce")
    df[col_qte] = (
        df[col_qte]
        .astype(str)
        .str.replace(",", "", regex=False)
        .str.replace("\u00a0", "", regex=False)
        .astype(float)
    )

    grouped = (
        df.groupby([col_article, col_date], as_index=False)[col_qte]
        .sum()
        .rename(columns={col_qte: "Quantit√©_totale"})
    )

    all_dates = pd.date_range(start=grouped[col_date].min(), end=grouped[col_date].max(), freq="D")
    all_articles = grouped[col_article].unique()
    full_index = pd.MultiIndex.from_product([all_articles, all_dates], names=[col_article, col_date])

    result = (
        grouped
        .set_index([col_article, col_date])
        .reindex(full_index, fill_value=0)
        .reset_index()
    )

    return result


def aggregate_quantities(df_daily, freq="D"):
    """Agr√®ge les quantit√©s par article sur la fr√©quence donn√©e."""
    if freq == "D":
        out = df_daily.copy()
        out = out.rename(columns={"Date de livraison": "P√©riode"})
        return out

    agg = (
        df_daily
        .groupby(["Description article", pd.Grouper(key="Date de livraison", freq=freq)])["Quantit√©_totale"]
        .sum()
        .reset_index()
        .rename(columns={"Date de livraison": "P√©riode"})
    )
    return agg


def working_days_between_fr(start_date, end_date):
    """
    Calcule la liste des jours ouvr√©s fran√ßais entre deux dates.
    Exclut les dimanches et les jours f√©ri√©s fran√ßais.
    Intervalle : start_date exclue ‚Üí end_date incluse.
    """
    # Normalisation
    if isinstance(start_date, datetime):
        start = start_date.date()
    else:
        start = start_date

    if isinstance(end_date, datetime):
        end = end_date.date()
    else:
        end = end_date

    # S'assure que start <= end
    if end < start:
        start, end = end, start

    # Jours f√©ri√©s FR pour toutes les ann√©es couvertes
    years = set(range(start.year, end.year + 1))
    fr_holidays = holidays.country_holidays("FR", years=years)
    working_days = []
    current = start

    while current < end:
        current += timedelta(days=1)

        # Dimanche
        if current.weekday() == 6:
            continue

        # Jour f√©ri√©
        if current in fr_holidays:
            continue

        working_days.append(current)

    return working_days


def periods_in_days_fr(start_date):
    """
    Donne pour diff√©rentes p√©riodes :
      - la date finale
      - le nombre total de jours calendaires
      - le nombre de jours ouvr√©s fran√ßais
    """
    # Normalisation
    if isinstance(start_date, datetime):
        start = start_date.date()
    else:
        start = start_date

    periods = {
        "1_semaine": {"weeks": 1},
        "1_mois": {"months": 1},
        "3_mois": {"months": 3},
        "6_mois": {"months": 6},
        "9_mois": {"months": 9},
    }

    results = {}

    for label, delta_kwargs in periods.items():
        end = start + relativedelta(**delta_kwargs)

        jours_calendaires = (end - start).days
        jours_ouvres = working_days_between_fr(start, end)

        results[label] = {
            "date_fin": end,
            "jours_calendaires": jours_calendaires,
            "jours_ouvres_fr": len(jours_ouvres),
        }

    return results


def keep_business_day(df_agg):
    """
    Filtre le DataFrame pour ne garder que les dates avec quantit√© > 0.
    √âlimine les jours non-ouvr√©s (z√©ros) du dataset.
    """
    # 1. Calculer la quantit√© totale par date
    somme_par_date = df_agg.groupby("P√©riode")["Quantit√©_totale"].sum()

    # 2. Garder uniquement les dates dont la somme est > 0
    dates_valides = somme_par_date[somme_par_date > 0].index

    # 3. Filtrer le DataFrame final
    df_filtre = df_agg[df_agg["P√©riode"].isin(dates_valides)].copy()
    return df_filtre


def call_modal_api(series_data, horizon, dates=None, product_name="Unknown", timeout=900):
    """
    Appelle l'API Modal pour obtenir des pr√©visions.

    Args:
        series_data: Donn√©es de la s√©rie temporelle
        horizon: Horizon de pr√©vision
        dates: Dates optionnelles
        product_name: Nom du produit
        timeout: Timeout en secondes (d√©faut: 900s = 15min pour batch)
    """
    payload = {
        "product_name": product_name,
        "series": series_data.tolist() if isinstance(series_data, np.ndarray) else list(series_data),
        "horizon": horizon,
    }

    if dates is not None:
        payload["dates"] = [d.isoformat() if hasattr(d, 'isoformat') else str(d) for d in dates]

    try:
        response = requests.post(MODAL_API_URL, json=payload, timeout=timeout)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.Timeout:
        logger.error(f"API Timeout for {product_name} after {timeout}s")
        return {"success": False, "error": f"Timeout apr√®s {timeout}s - l'API n'a pas r√©pondu √† temps"}
    except requests.exceptions.RequestException as e:
        logger.error(f"API Error for {product_name}: {e}")
        return {"success": False, "error": str(e)}


def create_forecast_excel_with_sum(forecast_df, product_name):
    """Cr√©e un fichier Excel avec ligne de somme."""
    # Ajouter ligne de somme
    sum_row = {}
    for col in forecast_df.columns:
        if col == "Date":
            sum_row[col] = "TOTAL"
        elif pd.api.types.is_numeric_dtype(forecast_df[col]):
            sum_row[col] = forecast_df[col].sum()
        else:
            sum_row[col] = ""

    df_with_sum = pd.concat([forecast_df, pd.DataFrame([sum_row])], ignore_index=True)

    buffer = io.BytesIO()
    with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
        df_with_sum.to_excel(writer, sheet_name="Pr√©visions", index=False)

        # Formater la derni√®re ligne (somme) en gras
        workbook = writer.book
        worksheet = writer.sheets["Pr√©visions"]

        from openpyxl.styles import Font
        last_row = len(df_with_sum) + 1
        for cell in worksheet[last_row]:
            cell.font = Font(bold=True)

    buffer.seek(0)
    return buffer


# ============================================================================
# PROFESSIONAL UI FUNCTIONS
# ============================================================================

def inject_custom_css():
    """Inject custom CSS for professional appearance"""
    st.markdown("""
        <style>
        /* Main app styling */
        .main {
            background: linear-gradient(to bottom, #f8f9fa 0%, #e9ecef 100%);
        }

        /* Professional header */
        .app-header {
            background: linear-gradient(135deg, #2c3e50 0%, #3498db 50%, #2980b9 100%);
            padding: 2.5rem 2rem;
            border-radius: 12px;
            margin-bottom: 2rem;
            box-shadow: 0 8px 16px rgba(0, 0, 0, 0.15);
            border: 1px solid rgba(255, 255, 255, 0.1);
        }

        .app-header h1 {
            color: white;
            margin: 0;
            font-size: 2.5rem;
            font-weight: 700;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);
            letter-spacing: -0.5px;
        }

        .app-header p {
            color: rgba(255, 255, 255, 0.95);
            margin: 0.75rem 0 0 0;
            font-size: 1.1rem;
            font-weight: 400;
        }

        /* Sidebar styling */
        [data-testid="stSidebar"] {
            background: linear-gradient(to bottom, #ffffff 0%, #f8f9fa 100%);
        }

        /* Button enhancements */
        .stButton>button {
            border-radius: 8px;
            font-weight: 600;
            transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
            border: none;
            padding: 0.6rem 1.5rem;
        }

        .stButton>button:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(0, 0, 0, 0.15);
        }

        .stButton>button[kind="primary"] {
            background: linear-gradient(135deg, #3498db 0%, #2980b9 100%);
        }

        .stButton>button[kind="primary"]:hover {
            background: linear-gradient(135deg, #2980b9 0%, #21618c 100%);
        }

        /* Tabs styling */
        .stTabs [data-baseweb="tab-list"] {
            gap: 4px;
            background-color: #f1f3f5;
            padding: 0.5rem;
            border-radius: 10px;
        }

        .stTabs [data-baseweb="tab"] {
            border-radius: 8px;
            padding: 0.75rem 1.5rem;
            font-weight: 600;
            background-color: transparent;
            transition: all 0.2s ease;
        }

        .stTabs [data-baseweb="tab"]:hover {
            background-color: rgba(52, 152, 219, 0.1);
        }

        .stTabs [aria-selected="true"] {
            background: white !important;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        /* Progress bar */
        .stProgress > div > div > div > div {
            background: linear-gradient(90deg, #3498db 0%, #2ecc71 100%);
        }

        /* Info boxes */
        .info-box {
            background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%);
            border-left: 4px solid #2196f3;
            padding: 1.25rem;
            border-radius: 8px;
            margin: 1rem 0;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
        }

        .info-box strong {
            color: #1565c0;
            font-size: 1.05rem;
        }

        /* Metric cards */
        [data-testid="stMetricValue"] {
            font-size: 1.8rem;
            font-weight: 700;
            color: #2c3e50;
        }

        [data-testid="stMetricLabel"] {
            font-weight: 600;
            color: #5a6c7d;
            text-transform: uppercase;
            font-size: 0.85rem;
            letter-spacing: 0.5px;
        }

        /* Dataframe styling */
        .stDataFrame {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08);
        }

        /* Input fields */
        .stTextInput>div>div>input,
        .stSelectbox>div>div>div,
        .stMultiselect>div>div>div {
            border-radius: 8px;
            border: 2px solid #e0e0e0;
            transition: all 0.2s ease;
        }

        .stTextInput>div>div>input:focus,
        .stSelectbox>div>div>div:focus,
        .stMultiselect>div>div>div:focus {
            border-color: #3498db;
            box-shadow: 0 0 0 2px rgba(52, 152, 219, 0.2);
        }

        /* Date input */
        .stDateInput>div>div>input {
            border-radius: 8px;
            border: 2px solid #e0e0e0;
        }

        /* Expander */
        .streamlit-expanderHeader {
            background-color: #f8f9fa;
            border-radius: 8px;
            font-weight: 600;
            border: 1px solid #e9ecef;
        }

        /* Success/Warning/Error messages */
        .stSuccess, .stWarning, .stError, .stInfo {
            border-radius: 8px;
            padding: 1rem;
            margin: 1rem 0;
        }

        /* Footer */
        .app-footer {
            margin-top: 4rem;
            padding: 2.5rem 2rem;
            text-align: center;
            color: #7f8c8d;
            border-top: 2px solid #e9ecef;
            background: linear-gradient(to top, #f8f9fa 0%, transparent 100%);
        }

        .app-footer p {
            margin: 0.5rem 0;
        }

        /* Horizontal line */
        hr {
            margin: 2rem 0;
            border: none;
            border-top: 2px solid #e9ecef;
        }

        /* Radio buttons */
        .stRadio>div {
            background-color: #f8f9fa;
            padding: 0.75rem;
            border-radius: 8px;
        }

        /* File uploader */
        [data-testid="stFileUploader"] {
            border: 2px dashed #3498db;
            border-radius: 10px;
            padding: 2rem;
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            transition: all 0.3s ease;
        }

        [data-testid="stFileUploader"]:hover {
            border-color: #2980b9;
            background: linear-gradient(135deg, #e9ecef 0%, #dee2e6 100%);
        }

        /* Subheaders */
        .stApp h2, .stApp h3 {
            color: #2c3e50;
            font-weight: 700;
        }

        /* Caption text */
        .stApp .stCaption {
            color: #6c757d;
            font-weight: 500;
        }

        /* Download button special styling */
        .stDownloadButton>button {
            background: linear-gradient(135deg, #27ae60 0%, #229954 100%);
            color: white;
        }

        .stDownloadButton>button:hover {
            background: linear-gradient(135deg, #229954 0%, #1e8449 100%);
        }
        </style>
    """, unsafe_allow_html=True)


def render_header():
    """Render professional header"""
    st.markdown("""
        <div class="app-header">
            <h1>Plateforme de Pr√©vision Entreprise</h1>
            <p>Syst√®me de pr√©vision de demande aliment√© par IA ‚Ä¢ Version 2.0.0</p>
        </div>
    """, unsafe_allow_html=True)


def render_footer():
    """Render professional footer"""
    st.markdown("""
        <div class="app-footer">
            <p style="font-size: 1.1rem; font-weight: 700; color: #2c3e50; margin-bottom: 0.5rem;">
                Luna Analytics Platform
            </p>
            <p style="font-size: 0.9rem; color: #7f8c8d; margin-bottom: 1rem;">
                Plateforme de pr√©vision de demande aliment√©e par intelligence artificielle
            </p>
            <p style="font-size: 0.85rem; color: #95a5a6;">
                Mod√®les: LSTM ‚Ä¢ Intermittent Forecaster ‚Ä¢ Sparse Spike Detection
            </p>
            <p style="font-size: 0.8rem; color: #bdc3c7; margin-top: 1rem;">
                ¬© 2025 Luna Analytics ‚Ä¢ Version 2.0.0
            </p>
        </div>
    """, unsafe_allow_html=True)


# =========================
# Interface Streamlit
# =========================

st.set_page_config(
    page_title="Luna Analytics ‚Ä¢ Enterprise Forecasting",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Apply professional styling
inject_custom_css()
render_header()

# Sidebar info
with st.sidebar:
    st.markdown("### Guide d'utilisation")
    st.markdown("""
    **√âtapes:**
    1. Importer vos donn√©es
    2. S√©lectionner les articles
    3. Configurer les param√®tres
    4. G√©n√©rer les pr√©visions
    5. T√©l√©charger les r√©sultats
    """)
    st.markdown("---")
    st.markdown("### Configuration syst√®me")
    st.caption(f"Points de donn√©es minimum: **{DATA_MIN}**")
    st.caption(f"Timeout API: **900 secondes**")

# Main content
st.markdown("""
    <div class="info-box">
        <strong>Format de donn√©es requis</strong><br>
        Fichier CSV (s√©parateur ;) ou Excel contenant les colonnes suivantes:
        <ul style="margin-top: 8px; margin-bottom: 0;">
            <li><code>Description article</code></li>
            <li><code>Date de livraison</code></li>
            <li><code>Quantite</code></li>
        </ul>
    </div>
""", unsafe_allow_html=True)

uploaded_file = st.file_uploader("S√©lectionner un fichier de donn√©es", type=["csv", "xlsx"])

if uploaded_file is not None:
    # Lecture du fichier
    if uploaded_file.name.lower().endswith(".csv"):
        df_raw = pd.read_csv(uploaded_file, sep=";")
    else:
        df_raw = pd.read_excel(uploaded_file)

    st.success("Fichier charg√© avec succ√®s")

    with st.expander("Aper√ßu des donn√©es brutes", expanded=False):
        st.dataframe(df_raw.head(10), use_container_width=True)

    # Pr√©paration du DataFrame journalier
    df_daily = prepare_daily_df(df_raw)

    # ==========
    # Classement des produits
    # ==========
    st.markdown("---")
    st.subheader("Classement des produits par quantit√© mensuelle cumul√©e")

    df_monthly_all = aggregate_quantities(df_daily, freq="M")
    ranking = (
        df_monthly_all
        .groupby("Description article")["Quantit√©_totale"]
        .sum()
        .sort_values(ascending=False)
        .reset_index()
        .rename(columns={"Quantit√©_totale": "Quantit√©_mensuelle_cumul√©e"})
    )

    st.dataframe(ranking, use_container_width=True)

    # ==========
    # ONGLETS : Article Unique vs Batch vs Validation
    # ==========
    st.markdown("---")
    tab1, tab2, tab3 = st.tabs([
        "Pr√©vision Article Unique",
        "Pr√©vision Batch (Multiples Articles)",
        "Validation Historique (Backtesting)"
    ])

    # ========================================
    # TAB 1 : ARTICLE UNIQUE
    # ========================================
    with tab1:
        st.subheader("Analyse d√©taill√©e par article")

        articles_sorted = ranking["Description article"].tolist()

        # Recherche
        search_text = st.text_input(
            "Rechercher un article",
            value="",
            placeholder="Tapez pour rechercher (ex: VIVA, LINDT, PATES...)",
            key="search_single"
        )

        if search_text:
            filtered_articles = [a for a in articles_sorted if search_text.lower() in a.lower()]
        else:
            filtered_articles = articles_sorted

        if not filtered_articles:
            st.warning("Aucun article ne correspond √† votre recherche.")
            st.stop()

        selected_article = st.selectbox("S√©lectionner un article", filtered_articles, key="select_single")

        freq_label = st.radio("Fr√©quence d'agr√©gation", ("Jour", "Semaine (Ne pas utiliser)"), horizontal=True, key="freq_single")

        if freq_label == "Jour":
            freq = "D"
        else:
            freq = "W-MON"

        df_agg_wo_bd = aggregate_quantities(df_daily, freq=freq)
        df_agg = keep_business_day(df_agg_wo_bd)
        df_article = df_agg[df_agg["Description article"] == selected_article].copy()
        df_article = df_article.sort_values("P√©riode")

        # Trimming des dates avec z√©ros
        nonzero_mask = df_article["Quantit√©_totale"] != 0
        if nonzero_mask.any():
            first_idx = df_article.index[nonzero_mask][0]
            last_idx = df_article.index[nonzero_mask][-1]
            df_article = df_article.loc[first_idx:last_idx]

        # S√©lection de fen√™tre temporelle et validation DATA_MIN
        if df_article.shape[0] > DATA_MIN:
            min_date = df_article["P√©riode"].min().date()
            max_date = df_article["P√©riode"].max().date()

            st.markdown("#### P√©riode d'analyse")
            col_start, col_end = st.columns(2)
            with col_start:
                start_date = st.date_input("Date de d√©but", value=min_date, min_value=min_date, max_value=max_date, key="start_single")
            with col_end:
                end_date = st.date_input("Date de fin", value=max_date, min_value=start_date, max_value=max_date, key="end_single")

            mask_window = (
                (df_article["P√©riode"] >= pd.to_datetime(start_date)) &
                (df_article["P√©riode"] <= pd.to_datetime(end_date))
            )
            df_article = df_article.loc[mask_window].copy()

            if df_article.empty:
                st.warning("La fen√™tre de dates choisie ne contient aucune donn√©e.")
                st.stop()
        else:
            st.warning(f"Donn√©es insuffisantes pour cet article (minimum {DATA_MIN} points requis).")
            st.stop()

        col_info1, col_info2 = st.columns(2)
        with col_info1:
            st.metric("Article s√©lectionn√©", selected_article)
        with col_info2:
            st.metric("Points de donn√©es", len(df_article))

        st.dataframe(df_article, use_container_width=True)

        # Graphique historique
        st.markdown("---")
        st.subheader("Historique des quantit√©s")

        series_hist = df_article.set_index("P√©riode")["Quantit√©_totale"]

        fig_hist = go.Figure()
        fig_hist.add_trace(
            go.Scatter(
                x=series_hist.index,
                y=series_hist.values,
                mode="lines",
                name="Historique",
                line=dict(color="black", width=1.5),
            )
        )

        fig_hist.update_layout(
            template="plotly_white",
            height=400,
            margin=dict(l=40, r=20, t=40, b=40),
            xaxis_title="Date",
            yaxis_title="Quantit√©",
            legend=dict(x=0.01, y=0.99),
        )

        st.plotly_chart(fig_hist, use_container_width=True)

        # Export Excel historique avec somme
        hist_df = series_hist.to_frame(name="Quantit√©_totale").reset_index()
        hist_buffer = create_forecast_excel_with_sum(hist_df, selected_article)

        st.download_button(
            label="T√©l√©charger l'historique (Excel)",
            data=hist_buffer,
            file_name=f"historique_{selected_article}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            key="download_hist"
        )

        # Pr√©vision IA
        st.markdown("---")
        st.subheader("Pr√©vision par intelligence artificielle")

        # S√©lection p√©riode de forecast avec jours ouvr√©s fran√ßais
        min_date_forecast = df_article["P√©riode"].min().date()
        max_date_forecast = df_article["P√©riode"].max().date()

        st.markdown("#### P√©riode de pr√©vision")
        col_forecast_start, col_forecast_end = st.columns(2)
        with col_forecast_start:
            forecast_start_date = st.date_input(
                "Date de d√©but",
                value=max_date_forecast,
                min_value=min_date_forecast,
                max_value=max_date_forecast,
                key="forecast_start_single"
            )
        with col_forecast_end:
            forecast_end_date = st.date_input(
                "Date de fin",
                value=max_date_forecast,
                min_value=max_date_forecast,
                max_value=max_date_forecast + relativedelta(years=1),
                key="forecast_end_single"
            )

        # Calcul automatique des jours ouvr√©s fran√ßais
        list_dates_business_day = working_days_between_fr(forecast_start_date, forecast_end_date)
        forecast_horizon = len(list_dates_business_day)

        if forecast_horizon > 0:
            st.info(f"Horizon calcul√©: **{forecast_horizon} jours ouvr√©s** fran√ßais (hors dimanches et jours f√©ri√©s)")
        else:
            st.warning("Aucun jour ouvr√© dans la p√©riode s√©lectionn√©e.")
            forecast_horizon = None

        run_forecast = st.button("Lancer la pr√©vision", key="run_single", type="primary")

        if forecast_horizon is not None and run_forecast:
            with st.spinner("G√©n√©ration de la pr√©vision en cours..."):
                result = call_modal_api(
                    series_data=series_hist.values,
                    horizon=forecast_horizon,
                    dates=series_hist.index,
                    product_name=selected_article
                )
                # Stocker dans session_state
                st.session_state.single_forecast_result = {
                    'result': result,
                    'series_hist': series_hist,
                    'forecast_horizon': forecast_horizon,
                    'selected_article': selected_article,
                    'future_index': list_dates_business_day  # Utiliser les jours ouvr√©s
                }

        # Afficher depuis session_state si disponible
        if 'single_forecast_result' in st.session_state:
            stored = st.session_state.single_forecast_result
            result = stored['result']
            series_hist = stored['series_hist']
            forecast_horizon = stored['forecast_horizon']
            selected_article = stored['selected_article']
            future_index = stored.get('future_index', [])  # Utiliser les jours ouvr√©s stock√©s

            if result and result.get("success"):
                st.success(f"Pr√©vision r√©ussie avec le mod√®le: **{result['model_used']}**")

                # Affichage diagnostics
                st.caption("Diagnostics du mod√®le:")
                routing_info = result.get("routing_info", {})
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Ratio de z√©ros", f"{routing_info.get('zero_ratio', 0)*100:.1f}%")
                with col2:
                    st.metric("Dispersion", f"{routing_info.get('dispersion', 0):.3f}")
                with col3:
                    st.metric("Autocorr√©lation", f"{routing_info.get('acf_lag1', 0):.3f}")

                # Extraction des r√©sultats
                predictions = np.array(result["predictions"])
                lower_bound = np.array(result["lower_bound"])
                upper_bound = np.array(result["upper_bound"])
                simulated_path = np.array(result["simulated_path"])
                median_predictions = result.get("median_predictions")

                # Graphique historique + pr√©visions
                st.subheader("R√©sultats de la pr√©vision")

                fig_pred = go.Figure()

                # Historique
                fig_pred.add_trace(
                    go.Scatter(
                        x=series_hist.index,
                        y=series_hist.values,
                        mode="lines",
                        name="Historique",
                        line=dict(color="black", width=1.5),
                    )
                )

                # Pr√©vision moyenne
                fig_pred.add_trace(
                    go.Scatter(
                        x=future_index,
                        y=predictions,
                        mode="lines",
                        name="Pr√©vision (moyenne)",
                        line=dict(color="blue", width=2),
                    )
                )

                # Intervalle de confiance
                fig_pred.add_trace(
                    go.Scatter(
                        x=future_index,
                        y=upper_bound,
                        mode="lines",
                        name="IC 95% (haut)",
                        line=dict(color="rgba(0,100,255,0.3)", width=1, dash="dot"),
                        showlegend=False,
                    )
                )

                fig_pred.add_trace(
                    go.Scatter(
                        x=future_index,
                        y=lower_bound,
                        mode="lines",
                        name="IC 95%",
                        line=dict(color="rgba(0,100,255,0.3)", width=1, dash="dot"),
                        fill="tonexty",
                        fillcolor="rgba(0,100,255,0.2)",
                    )
                )

                # M√©diane si disponible
                if median_predictions is not None:
                    fig_pred.add_trace(
                        go.Scatter(
                            x=future_index,
                            y=median_predictions,
                            mode="lines",
                            name="Pr√©vision (m√©diane)",
                            line=dict(color="green", width=2, dash="dash"),
                        )
                    )

                # Trajectoire simul√©e
                if result["model_used"] == "BayesianLSTM":
                    label = "Trajectoire simul√©e (MC Dropout)"
                    color = "rgba(124, 252, 0, 0.9)"
                elif result["model_used"] == "SparseSpikeForecaster":
                    label = "Pics p√©riodiques simul√©s"
                    color = "rgba(255, 165, 0, 0.9)"
                else:
                    label = "Sc√©nario simul√© 0/spikes"
                    color = "rgba(255, 0, 0, 0.9)"

                fig_pred.add_trace(
                    go.Scatter(
                        x=future_index,
                        y=simulated_path,
                        mode="markers+lines",
                        name=label,
                        line=dict(color=color, width=1.5),
                        marker=dict(size=6),
                    )
                )

                fig_pred.update_layout(
                    template="plotly_white",
                    height=500,
                    xaxis_title="Temps",
                    yaxis_title="Quantit√©",
                    legend=dict(x=0.01, y=0.99),
                    title=f"Pr√©visions H={forecast_horizon} - {result['model_used']}",
                )

                st.plotly_chart(fig_pred, use_container_width=True)

                # Export Excel pr√©visions avec somme
                forecast_df = pd.DataFrame({
                    "Date": future_index,
                    "Pr√©vision_moyenne": predictions,
                    "IC_95_bas": lower_bound,
                    "IC_95_haut": upper_bound,
                    "Trajectoire_simul√©e": simulated_path,
                })

                if median_predictions is not None:
                    forecast_df["Pr√©vision_m√©diane"] = median_predictions

                forecast_buffer = create_forecast_excel_with_sum(forecast_df, selected_article)

                st.download_button(
                    label="T√©l√©charger les pr√©visions (Excel)",
                    data=forecast_buffer,
                    file_name=f"previsions_{selected_article}_H{forecast_horizon}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    key="download_forecast_single",
                    type="primary"
                )

            elif result:
                st.error(f"Erreur lors de la pr√©vision: {result.get('error', 'Erreur inconnue')}")

    # ========================================
    # TAB 2 : BATCH FORECAST
    # ========================================
    with tab2:
        st.subheader("Pr√©vision Batch - Multiples Articles")
        st.markdown("G√©n√©rez des pr√©visions pour plusieurs articles simultan√©ment et t√©l√©chargez l'ensemble des r√©sultats.")

        # S√©lection des articles
        batch_search = st.text_input(
            "Filtrer les articles",
            value="",
            placeholder="Tapez pour filtrer...",
            key="search_batch"
        )

        articles_sorted = ranking["Description article"].tolist()
        if batch_search:
            filtered_batch = [a for a in articles_sorted if batch_search.lower() in a.lower()]
        else:
            filtered_batch = articles_sorted

        selected_articles = st.multiselect(
            "S√©lectionner les articles",
            filtered_batch,
            default=[],
            key="select_batch",
            help="S√©lectionnez un ou plusieurs articles pour la pr√©vision batch"
        )

        if len(selected_articles) > 0:
            st.caption(f"**{len(selected_articles)}** article(s) s√©lectionn√©(s)")

        # Avertissement pour les gros batchs
        if len(selected_articles) > 10:
            st.warning(
                f"**Traitement volumineux d√©tect√©** ({len(selected_articles)} articles)\n\n"
                "‚Ä¢ Temps estim√©: ~" + str(len(selected_articles) * 3) + " minutes\n"
                "‚Ä¢ Ne fermez pas cette page pendant le traitement\n"
                "‚Ä¢ Les r√©sultats seront automatiquement sauvegard√©s"
            )

        # Param√®tres batch
        st.markdown("---")
        batch_freq = st.radio("Fr√©quence d'agr√©gation", ("Jour", "Semaine (Ne pas utiliser)"), horizontal=True, key="freq_batch")

        # S√©lection de plage de dates pour l'historique
        st.markdown("#### P√©riode historique")

        # Obtenir min/max dates globales
        if len(selected_articles) > 0:
            temp_freq = "D" if batch_freq == "Jour" else "W-MON"
            df_temp = aggregate_quantities(df_daily, freq=temp_freq)
            all_dates = df_temp["P√©riode"].unique()
            global_min_date = pd.to_datetime(all_dates).min().date()
            global_max_date = pd.to_datetime(all_dates).max().date()
        else:
            global_min_date = df_daily["Date de livraison"].min().date()
            global_max_date = df_daily["Date de livraison"].max().date()

        col_batch_start, col_batch_end = st.columns(2)
        with col_batch_start:
            batch_start_date = st.date_input(
                "Date de d√©but",
                value=global_min_date,
                min_value=global_min_date,
                max_value=global_max_date,
                key="batch_start_date"
            )
        with col_batch_end:
            batch_end_date = st.date_input(
                "Date de fin",
                value=global_max_date,
                min_value=batch_start_date,
                max_value=global_max_date,
                key="batch_end_date"
            )

        # S√©lection p√©riode de forecast avec jours ouvr√©s
        st.markdown("#### P√©riode de pr√©vision")

        col_forecast_batch_start, col_forecast_batch_end = st.columns(2)
        with col_forecast_batch_start:
            forecast_batch_start_date = st.date_input(
                "Date de d√©but",
                value=global_max_date,
                min_value=global_min_date,
                max_value=global_max_date,
                key="forecast_batch_start_date"
            )
        with col_forecast_batch_end:
            forecast_batch_end_date = st.date_input(
                "Date de fin",
                value=global_max_date,
                min_value=global_max_date,
                max_value=global_max_date + relativedelta(years=1),
                key="forecast_batch_end_date"
            )

        # Calcul automatique des jours ouvr√©s fran√ßais
        list_dates_batch_business_day = working_days_between_fr(forecast_batch_start_date, forecast_batch_end_date)
        horizon_batch_val = len(list_dates_batch_business_day)

        if horizon_batch_val > 0:
            st.info(f"Horizon calcul√©: **{horizon_batch_val} jours ouvr√©s** fran√ßais (hors dimanches et jours f√©ri√©s)")
        else:
            st.warning("Aucun jour ouvr√© dans la p√©riode s√©lectionn√©e.")

        if batch_freq == "Jour":
            freq_batch_val = "D"
        else:
            freq_batch_val = "W-MON"

        st.markdown("---")
        run_batch = st.button("Lancer la pr√©vision batch", key="run_batch", type="primary")

        if run_batch and len(selected_articles) > 0 and horizon_batch_val > 0:
            st.info(f"Traitement de {len(selected_articles)} article(s) en cours...")

            # Initialiser stockage des r√©sultats
            st.session_state.batch_results = {}  # Reset
            st.session_state.all_forecasts = []  # Reset
            st.session_state.batch_config = {
                'freq': freq_batch_val,
                'horizon': horizon_batch_val,
                'start_date': batch_start_date,
                'end_date': batch_end_date,
                'future_index': list_dates_batch_business_day  # Stocker les jours ouvr√©s
            }

            progress_bar = st.progress(0)
            status_text = st.empty()

            all_forecasts = []

            failed_articles = []
            success_count = 0

            for idx, article in enumerate(selected_articles):
                # Mise √† jour statut d√©taill√© pour maintenir la connexion
                status_text.text(f"[{idx+1}/{len(selected_articles)}] Traitement: {article}")
                progress_bar.progress((idx) / len(selected_articles))

                try:
                    # Pr√©parer donn√©es avec filtrage business days
                    df_agg_batch_wo_bd = aggregate_quantities(df_daily, freq=freq_batch_val)
                    df_agg_batch = keep_business_day(df_agg_batch_wo_bd)
                    df_art = df_agg_batch[df_agg_batch["Description article"] == article].copy()
                    df_art = df_art.sort_values("P√©riode")

                    # Trimming
                    nonzero_mask = df_art["Quantit√©_totale"] != 0
                    if nonzero_mask.any():
                        first_idx = df_art.index[nonzero_mask][0]
                        last_idx = df_art.index[nonzero_mask][-1]
                        df_art = df_art.loc[first_idx:last_idx]

                    # Apply date range filter
                    mask_batch_window = (
                        (df_art["P√©riode"] >= pd.to_datetime(batch_start_date)) &
                        (df_art["P√©riode"] <= pd.to_datetime(batch_end_date))
                    )
                    df_art = df_art.loc[mask_batch_window].copy()

                    if df_art.empty:
                        st.warning(f"Pas de donn√©es pour {article}, ignor√©.")
                        failed_articles.append((article, "Pas de donn√©es"))
                        continue

                    if df_art.shape[0] < DATA_MIN:
                        st.warning(f"Donn√©es insuffisantes pour {article} ({df_art.shape[0]} < {DATA_MIN}), ignor√©.")
                        failed_articles.append((article, f"Insuffisant ({df_art.shape[0]} points)"))
                        continue

                    series_data = df_art.set_index("P√©riode")["Quantit√©_totale"]

                    # Mise √† jour statut - Appel API
                    status_text.text(f"[{idx+1}/{len(selected_articles)}] {article} - Appel API...")

                    # Appel API avec timeout adapt√©
                    result = call_modal_api(
                        series_data=series_data.values,
                        horizon=horizon_batch_val,
                        dates=series_data.index,
                        product_name=article,
                        timeout=900  # 15 minutes par article
                    )

                    if result and result.get("success"):
                        # Stocker r√©sultat
                        st.session_state.batch_results[article] = result
                        success_count += 1

                        # Utiliser les jours ouvr√©s fran√ßais comme future_index
                        future_index = list_dates_batch_business_day

                        # Cr√©er DataFrame pr√©vision
                        forecast_df = pd.DataFrame({
                            "Article": article,
                            "Date": future_index,
                            "Pr√©vision_moyenne": result["predictions"],
                            "IC_95_bas": result["lower_bound"],
                            "IC_95_haut": result["upper_bound"],
                            "Trajectoire_simul√©e": result["simulated_path"],
                            "Mod√®le": result["model_used"]
                        })

                        if result.get("median_predictions"):
                            forecast_df["Pr√©vision_m√©diane"] = result["median_predictions"]

                        all_forecasts.append(forecast_df)
                        status_text.text(f"[{idx+1}/{len(selected_articles)}] {article} - Termin√©")

                    else:
                        error_msg = result.get('error', 'Erreur inconnue') if result else 'Pas de r√©ponse'
                        st.warning(f"√âchec pour {article}: {error_msg}")
                        failed_articles.append((article, error_msg))

                except Exception as e:
                    st.error(f"Erreur lors du traitement de {article}: {str(e)}")
                    failed_articles.append((article, str(e)))
                    logger.exception(f"Batch error for {article}")

                progress_bar.progress((idx + 1) / len(selected_articles))

            # Stocker all_forecasts dans session_state
            st.session_state.all_forecasts = all_forecasts

            # Message de fin d√©taill√©
            progress_bar.progress(1.0)
            status_text.empty()

            if success_count == len(selected_articles):
                st.success(f"Traitement termin√© avec succ√®s: {success_count}/{len(selected_articles)} articles")
            elif success_count > 0:
                st.warning(f"Traitement partiel: {success_count}/{len(selected_articles)} articles r√©ussis")
            else:
                st.error(f"√âchec complet: aucun article n'a pu √™tre trait√©")

            # Afficher les articles √©chou√©s si pr√©sents
            if failed_articles:
                with st.expander(f"Articles √©chou√©s ({len(failed_articles)})", expanded=False):
                    for art, reason in failed_articles:
                        st.text(f"‚Ä¢ {art}: {reason}")

        # Afficher depuis session_state si disponible
        if 'all_forecasts' in st.session_state and len(st.session_state.all_forecasts) > 0:
            all_forecasts = st.session_state.all_forecasts
            freq_batch_val = st.session_state.batch_config['freq']
            horizon_batch_val = st.session_state.batch_config['horizon']
            future_index_batch = st.session_state.batch_config.get('future_index', [])

            if True:  # Always display if we have results
                st.markdown("---")
                st.subheader("R√©sum√© des pr√©visions")

                summary_data = []
                for article, res in st.session_state.batch_results.items():
                    summary_data.append({
                        "Article": article,
                        "Mod√®le utilis√©": res["model_used"],
                        "Total pr√©vu (moyenne)": sum(res["predictions"]),
                        "Zero ratio": f"{res['routing_info']['zero_ratio']*100:.1f}%"
                    })

                summary_df = pd.DataFrame(summary_data)
                st.dataframe(summary_df, use_container_width=True)

                # Visualisation individuelle par article
                st.markdown("---")
                st.subheader("Visualisation par article")

                selected_viz_article = st.selectbox(
                    "S√©lectionnez un article pour voir son graphique :",
                    list(st.session_state.batch_results.keys()),
                    key="viz_article_select"
                )

                if selected_viz_article:
                    viz_result = st.session_state.batch_results[selected_viz_article]

                    # R√©cup√©rer les donn√©es historiques de cet article
                    df_agg_viz_wo_bd = aggregate_quantities(df_daily, freq=freq_batch_val)
                    df_agg_viz = keep_business_day(df_agg_viz_wo_bd)
                    df_art_viz = df_agg_viz[df_agg_viz["Description article"] == selected_viz_article].copy()
                    df_art_viz = df_art_viz.sort_values("P√©riode")

                    # Trimming
                    nonzero_mask_viz = df_art_viz["Quantit√©_totale"] != 0
                    if nonzero_mask_viz.any():
                        first_idx_viz = df_art_viz.index[nonzero_mask_viz][0]
                        last_idx_viz = df_art_viz.index[nonzero_mask_viz][-1]
                        df_art_viz = df_art_viz.loc[first_idx_viz:last_idx_viz]

                    series_viz = df_art_viz.set_index("P√©riode")["Quantit√©_totale"]

                    # Utiliser les jours ouvr√©s fran√ßais stock√©s
                    future_index_viz = future_index_batch

                    # Cr√©er graphique
                    fig_viz = go.Figure()

                    # Historique
                    fig_viz.add_trace(
                        go.Scatter(
                            x=series_viz.index,
                            y=series_viz.values,
                            mode="lines",
                            name="Historique",
                            line=dict(color="black", width=1.5),
                        )
                    )

                    # Pr√©vision moyenne
                    fig_viz.add_trace(
                        go.Scatter(
                            x=future_index_viz,
                            y=viz_result["predictions"],
                            mode="lines",
                            name="Pr√©vision (moyenne)",
                            line=dict(color="blue", width=2),
                        )
                    )

                    # IC
                    fig_viz.add_trace(
                        go.Scatter(
                            x=future_index_viz,
                            y=viz_result["upper_bound"],
                            mode="lines",
                            name="IC 95%",
                            line=dict(color="rgba(0,100,255,0.3)", width=1, dash="dot"),
                            showlegend=False,
                        )
                    )

                    fig_viz.add_trace(
                        go.Scatter(
                            x=future_index_viz,
                            y=viz_result["lower_bound"],
                            mode="lines",
                            name="IC 95%",
                            line=dict(color="rgba(0,100,255,0.3)", width=1, dash="dot"),
                            fill="tonexty",
                            fillcolor="rgba(0,100,255,0.2)",
                        )
                    )

                    # Trajectoire
                    if viz_result["model_used"] == "BayesianLSTM":
                        label_viz = "Trajectoire simul√©e (MC Dropout)"
                        color_viz = "rgba(124, 252, 0, 0.9)"
                    elif viz_result["model_used"] == "SparseSpikeForecaster":
                        label_viz = "Pics p√©riodiques simul√©s"
                        color_viz = "rgba(255, 165, 0, 0.9)"
                    else:
                        label_viz = "Sc√©nario simul√© 0/spikes"
                        color_viz = "rgba(255, 0, 0, 0.9)"

                    fig_viz.add_trace(
                        go.Scatter(
                            x=future_index_viz,
                            y=viz_result["simulated_path"],
                            mode="markers+lines",
                            name=label_viz,
                            line=dict(color=color_viz, width=1.5),
                            marker=dict(size=6),
                        )
                    )

                    fig_viz.update_layout(
                        template="plotly_white",
                        height=500,
                        xaxis_title="Temps",
                        yaxis_title="Quantit√©",
                        legend=dict(x=0.01, y=0.99),
                        title=f"{selected_viz_article} - {viz_result['model_used']}",
                    )

                    st.plotly_chart(fig_viz, use_container_width=True)

                    # T√©l√©chargement individuel
                    st.markdown("---")
                    st.caption(f"T√©l√©chargement des r√©sultats pour {selected_viz_article}")

                    forecast_df_viz = pd.DataFrame({
                        "Date": future_index_viz,
                        "Pr√©vision_moyenne": viz_result["predictions"],
                        "IC_95_bas": viz_result["lower_bound"],
                        "IC_95_haut": viz_result["upper_bound"],
                        "Trajectoire_simul√©e": viz_result["simulated_path"],
                    })

                    if viz_result.get("median_predictions"):
                        forecast_df_viz["Pr√©vision_m√©diane"] = viz_result["median_predictions"]

                    individual_buffer = create_forecast_excel_with_sum(forecast_df_viz, selected_viz_article)

                    st.download_button(
                        label=f"T√©l√©charger: {selected_viz_article}",
                        data=individual_buffer,
                        file_name=f"prevision_{selected_viz_article}_H{horizon_batch_val}.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                        key=f"download_individual_{selected_viz_article}"
                    )

                # T√©l√©chargement group√©
                st.markdown("---")
                st.subheader("T√©l√©chargement group√©")

                combined_df = pd.concat(all_forecasts, ignore_index=True)

                # Cr√©er Excel avec toutes les pr√©visions
                batch_buffer = io.BytesIO()
                with pd.ExcelWriter(batch_buffer, engine='openpyxl') as writer:
                    # Une feuille par article
                    for article in combined_df["Article"].unique():
                        article_df = combined_df[combined_df["Article"] == article].copy()
                        article_df = article_df.drop(columns=["Article"])

                        # Ajouter ligne somme
                        sum_row = {}
                        for col in article_df.columns:
                            if col == "Date":
                                sum_row[col] = "TOTAL"
                            elif col == "Mod√®le":
                                sum_row[col] = ""
                            elif pd.api.types.is_numeric_dtype(article_df[col]):
                                sum_row[col] = article_df[col].sum()
                            else:
                                sum_row[col] = ""

                        article_df_with_sum = pd.concat([article_df, pd.DataFrame([sum_row])], ignore_index=True)

                        # Nettoyer le nom de feuille (Excel interdit certains caract√®res)
                        sheet_name = article[:31]  # Excel limit
                        for char in ['\\', '/', '?', '*', '[', ']', ':']:
                            sheet_name = sheet_name.replace(char, '_')
                        sheet_name = sheet_name.strip("'")  # Pas d'apostrophe au d√©but/fin

                        article_df_with_sum.to_excel(writer, sheet_name=sheet_name, index=False)

                        # Formater derni√®re ligne
                        from openpyxl.styles import Font
                        worksheet = writer.sheets[sheet_name]
                        last_row = len(article_df_with_sum) + 1
                        for cell in worksheet[last_row]:
                            cell.font = Font(bold=True)

                    # Feuille de synth√®se
                    summary_df.to_excel(writer, sheet_name="Synth√®se", index=False)

                    # Feuille des totaux par produit
                    product_totals = []
                    for article in combined_df["Article"].unique():
                        article_data = combined_df[combined_df["Article"] == article]
                        product_totals.append({
                            "Article": article,
                            "Total_Pr√©vision_Moyenne": article_data["Pr√©vision_moyenne"].sum(),
                            "Total_IC_95_Bas": article_data["IC_95_bas"].sum(),
                            "Total_IC_95_Haut": article_data["IC_95_haut"].sum(),
                            "Total_Trajectoire_Simul√©e": article_data["Trajectoire_simul√©e"].sum(),
                            "Mod√®le": article_data["Mod√®le"].iloc[0] if len(article_data) > 0 else ""
                        })

                    totals_df = pd.DataFrame(product_totals)
                    totals_df.to_excel(writer, sheet_name="Totaux_par_Produit", index=False)

                batch_buffer.seek(0)

                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                st.download_button(
                    label=f"T√©l√©charger toutes les pr√©visions ({len(all_forecasts)} articles)",
                    data=batch_buffer,
                    file_name=f"batch_forecast_{timestamp}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    key="download_batch",
                    type="primary"
                )

        elif run_batch and len(selected_articles) == 0:
            st.warning("Veuillez s√©lectionner au moins un article.")
        elif run_batch and horizon_batch_val == 0:
            st.warning("Aucun jour ouvr√© dans la p√©riode s√©lectionn√©e. Veuillez choisir une p√©riode valide.")

    # ========================================
    # TAB 3 : VALIDATION HISTORIQUE (BACKTESTING)
    # ========================================
    with tab3:
        st.subheader("Validation Historique (Backtesting)")
        st.markdown(
            "Testez la pr√©cision du mod√®le en comparant ses pr√©dictions avec des donn√©es historiques r√©elles. "
            "Le mod√®le est entra√Æn√© sur une p√©riode et pr√©dit sur une autre p√©riode dont vous connaissez d√©j√† les r√©sultats."
        )

        # S√©lection articles multiples
        st.markdown("---")
        st.markdown("#### S√©lection des articles")

        search_text_val = st.text_input(
            "Rechercher des articles",
            value="",
            placeholder="Tapez pour rechercher (ex: VIVA, LINDT, PATES...)",
            key="search_validation"
        )

        if search_text_val:
            filtered_articles_val = [a for a in articles_sorted if search_text_val.lower() in a.lower()]
        else:
            filtered_articles_val = articles_sorted

        if not filtered_articles_val:
            st.warning("Aucun article ne correspond √† votre recherche.")
            st.stop()

        selected_articles_val = st.multiselect(
            "S√©lectionner les articles √† valider",
            filtered_articles_val,
            default=[],
            key="articles_validation"
        )

        if not selected_articles_val:
            st.info("S√©lectionnez au moins un article pour commencer")
            st.stop()

        st.caption(f"**{len(selected_articles_val)}** article(s) s√©lectionn√©(s)")

        # Fr√©quence
        st.markdown("---")
        freq_label_val = st.radio(
            "Fr√©quence d'agr√©gation",
            ("Jour", "Semaine (Ne pas utiliser)"),
            horizontal=True,
            key="freq_validation"
        )

        if freq_label_val == "Jour":
            freq_val = "D"
        else:
            freq_val = "W-MON"

        # Obtenir les dates globales pour tous les articles s√©lectionn√©s avec filtrage business days
        df_agg_val_wo_bd = aggregate_quantities(df_daily, freq=freq_val)
        df_agg_val = keep_business_day(df_agg_val_wo_bd)
        df_selected_val = df_agg_val[df_agg_val["Description article"].isin(selected_articles_val)].copy()

        if df_selected_val.empty:
            st.warning("Aucune donn√©e disponible pour les articles s√©lectionn√©s.")
            st.stop()

        # S√©lection des p√©riodes train/test
        st.markdown("#### D√©finition des p√©riodes")

        min_date_val = df_selected_val["P√©riode"].min().date()
        max_date_val = df_selected_val["P√©riode"].max().date()

        st.caption("**P√©riode d'entra√Ænement**")
        col_train_start, col_train_end = st.columns(2)
        with col_train_start:
            train_start_date = st.date_input(
                "Date de d√©but",
                value=min_date_val,
                min_value=min_date_val,
                max_value=max_date_val,
                key="train_start"
            )
        with col_train_end:
            train_end_date = st.date_input(
                "Date de fin",
                value=min_date_val + (max_date_val - min_date_val) * 0.7,  # 70% pour train
                min_value=train_start_date,
                max_value=max_date_val,
                key="train_end"
            )

        st.caption("**P√©riode de test**")
        col_test_start, col_test_end = st.columns(2)
        with col_test_start:
            test_start_date = st.date_input(
                "Date de d√©but",
                value=train_end_date + pd.Timedelta(days=1),
                min_value=train_end_date,
                max_value=max_date_val,
                key="test_start"
            )
        with col_test_end:
            test_end_date = st.date_input(
                "Date de fin",
                value=max_date_val,
                min_value=test_start_date,
                max_value=max_date_val,
                key="test_end"
            )

        # Bouton validation
        st.markdown("---")
        run_validation = st.button("Lancer la validation", key="run_validation", type="primary")

        if run_validation:
            st.info(f"Validation de {len(selected_articles_val)} article(s) en cours...")

            # Initialiser stockage
            st.session_state.validation_results = []
            st.session_state.validation_config = {
                'train_start': train_start_date,
                'train_end': train_end_date,
                'test_start': test_start_date,
                'test_end': test_end_date,
                'freq': freq_val
            }

            progress_bar = st.progress(0)
            status_text = st.empty()

            validation_summary = []

            for idx, article in enumerate(selected_articles_val):
                status_text.text(f"[{idx+1}/{len(selected_articles_val)}] Validation: {article}")

                # Pr√©parer donn√©es pour cet article
                df_article_val = df_agg_val[df_agg_val["Description article"] == article].copy()
                df_article_val = df_article_val.sort_values("P√©riode")

                # Trimming
                nonzero_mask_val = df_article_val["Quantit√©_totale"] != 0
                if nonzero_mask_val.any():
                    first_idx_val = df_article_val.index[nonzero_mask_val][0]
                    last_idx_val = df_article_val.index[nonzero_mask_val][-1]
                    df_article_val = df_article_val.loc[first_idx_val:last_idx_val]

                # Filtrer train
                mask_train = (
                    (df_article_val["P√©riode"] >= pd.to_datetime(train_start_date)) &
                    (df_article_val["P√©riode"] <= pd.to_datetime(train_end_date))
                )
                df_train = df_article_val.loc[mask_train].copy()

                # Filtrer test
                mask_test = (
                    (df_article_val["P√©riode"] >= pd.to_datetime(test_start_date)) &
                    (df_article_val["P√©riode"] <= pd.to_datetime(test_end_date))
                )
                df_test = df_article_val.loc[mask_test].copy()

                if df_train.empty or df_test.empty:
                    st.warning(f"Donn√©es insuffisantes pour {article}, ignor√©.")
                    continue

                # Pr√©parer s√©ries
                series_train = df_train.set_index("P√©riode")["Quantit√©_totale"]
                true_values = df_test.set_index("P√©riode")["Quantit√©_totale"].values
                horizon_val = len(df_test)

                # Appel API
                result_val = call_modal_api(
                    series_data=series_train.values,
                    horizon=horizon_val,
                    dates=series_train.index,
                    product_name=article
                )

                if result_val and result_val.get("success"):
                    predictions_val = np.array(result_val["predictions"])
                    lower_bound_val = np.array(result_val["lower_bound"])
                    upper_bound_val = np.array(result_val["upper_bound"])
                    simulated_path_val = np.array(result_val["simulated_path"])

                    # Calculer totaux et m√©triques
                    total_predicted = predictions_val.sum()
                    total_real = true_values.sum()
                    total_ic_bas = lower_bound_val.sum()
                    total_ic_haut = upper_bound_val.sum()
                    total_trajectoire = simulated_path_val.sum()

                    mae = np.mean(np.abs(predictions_val - true_values))
                    rmse = np.sqrt(np.mean((predictions_val - true_values) ** 2))

                    # MAPE
                    mask_nonzero = true_values != 0
                    if mask_nonzero.any():
                        mape = np.mean(np.abs((true_values[mask_nonzero] - predictions_val[mask_nonzero]) / true_values[mask_nonzero])) * 100
                    else:
                        mape = np.nan

                    validation_summary.append({
                        "Article": article,
                        "Total_Pr√©vision_Moyenne": total_predicted,
                        "Total_IC_95_Bas": total_ic_bas,
                        "Total_IC_95_Haut": total_ic_haut,
                        "Total_Trajectoire_Simul√©e": total_trajectoire,
                        "Total_R√©el": total_real,
                        "Erreur_Absolue": abs(total_predicted - total_real),
                        "Erreur_Relative_%": abs(total_predicted - total_real) / total_real * 100 if total_real != 0 else np.nan,
                        "MAE": mae,
                        "RMSE": rmse,
                        "MAPE_%": mape,
                        "Mod√®le": result_val["model_used"],
                        "Points_Train": len(series_train),
                        "Points_Test": len(df_test)
                    })
                else:
                    st.warning(f"√âchec pour {article}")

                progress_bar.progress((idx + 1) / len(selected_articles_val))

            # Stocker r√©sultats
            st.session_state.validation_results = validation_summary

            status_text.text("Validation termin√©e")
            st.success(f"Validation r√©ussie pour {len(validation_summary)}/{len(selected_articles_val)} article(s)")

        # Afficher r√©sultats depuis session_state
        if 'validation_results' in st.session_state and len(st.session_state.validation_results) > 0:
            validation_summary = st.session_state.validation_results
            validation_df = pd.DataFrame(validation_summary)

            st.markdown("---")
            st.subheader("R√©sultats de la validation")

            # Afficher m√©triques globales
            col1, col2, col3 = st.columns(3)
            with col1:
                avg_mae = validation_df["MAE"].mean()
                st.metric("MAE Moyenne", f"{avg_mae:.2f}")
            with col2:
                avg_rmse = validation_df["RMSE"].mean()
                st.metric("RMSE Moyenne", f"{avg_rmse:.2f}")
            with col3:
                avg_mape = validation_df["MAPE_%"].mean()
                if not np.isnan(avg_mape):
                    st.metric("MAPE Moyenne", f"{avg_mape:.2f}%")
                else:
                    st.metric("MAPE Moyenne", "N/A")

            # Afficher tableau synth√©tique
            st.markdown("---")
            st.subheader("Tableau synth√©tique")
            st.dataframe(validation_df, use_container_width=True)

            # Export Excel
            st.markdown("---")
            st.subheader("T√©l√©chargement")

            validation_buffer = io.BytesIO()
            with pd.ExcelWriter(validation_buffer, engine='openpyxl') as writer:
                # UNE SEULE feuille avec le tableau synth√©tique
                validation_df.to_excel(writer, sheet_name="Validation_Synth√®se", index=False)

                # Formater les nombres
                from openpyxl.styles import Font
                worksheet = writer.sheets["Validation_Synth√®se"]

                # En-t√™tes en gras
                for cell in worksheet[1]:
                    cell.font = Font(bold=True)

            validation_buffer.seek(0)

            timestamp_val = datetime.now().strftime("%Y%m%d_%H%M%S")
            st.download_button(
                label=f"T√©l√©charger les r√©sultats ({len(validation_summary)} articles)",
                data=validation_buffer,
                file_name=f"validation_batch_{timestamp_val}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                key="download_validation",
                type="primary"
            )


# Render footer
render_footer()
